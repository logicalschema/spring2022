---
title: "Data 622 Homework 2"
author: "Sung Lee"
date: '2022-03-24'
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: paper
---

# Assignment  
Based on the latest topics presented, bring a dataset of your choice and create a Decision Tree where you can solve a classification or regression problem and predict the outcome of a particular feature or detail of the data used. Switch variables to generate 2 decision trees and compare the results. Create a random forest for regression and analyze the results. Based on real cases where decision trees went wrong, and 'the bad & ugly' aspects of decision trees [https://decizone.com/blog/the-good-the-bad-the-ugly-of-using-decision-trees](https://decizone.com/blog/the-good-the-bad-the-ugly-of-using-decision-trees), how can you change this perception when using the decision tree you created to solve a real problem?
Format: document with screen captures & analysis.  


# Data  
For this assignment, I used the "Flavors of Cacao" dataset from this [https://www.kaggle.com/datasets/techiesid01/flavours-of-cacao?resource=download]( Kaggle website). With this dataset, I will create a decision tree that will solve a regression problem: what will be the final grade for a cocoa bean. The dataset will be split in a training and testing partitions.  

The following code sets up our libraries and imports the dataset.

```{r}
library(tidymodels)
library(tidyverse)
library(readr)
library(rpart.plot)

# Used for Random Forest
library(ranger)

chocolate <- read_csv('https://raw.githubusercontent.com/logicalschema/spring2022/main/data622/hw2/flavors_of_cacao.csv')

```  

This is a view of the imported dataset.

```{r}
names(chocolate)
head(chocolate)
```  

## Cleaning  
The column `cocoa_percent` has a `chr` data type so this needs to be converted to a double data type. The column `ref` was removed. The columns `broad_bean_origin` and `bean_type` were converted to `factor` variables. I also did a `drop_na`.


```{r}
chocolate <- chocolate %>% drop_na()


# Parses the cocoa_percent and converts it to percentage
chocolate$cocoa_percent <- parse_number(chocolate$cocoa_percent)/100

# Removes the ref column
chocolate <- chocolate[-c(3)]

# Convert some columns to factors
chocolate$broad_bean_origin  <- as.factor(chocolate$broad_bean_origin )
chocolate$bean_type  <- as.factor(chocolate$bean_type)

```  

This is a view of the data after our modification.  

```{r}
head(chocolate)
summary(chocolate)

```

## Partition  

```{r}
# Splitting the data 80/20
set.seed(32022)

data_split <- initial_split(chocolate, prop = 0.8, strata = 'bean_type')
chocolate_train <- training(data_split)
chocolate_test <- testing(data_split)

```  

# Regression Decision Tree  

## Constructing the First Model

I will construct the model for a Decision Tree regression tree using the training dataset.

```{r}

# Build the model specification for a decision tree
model_spec <- decision_tree() %>%
  set_mode("regression") %>%
  set_engine("rpart")

model_spec

```  

### Fit the Data  
The following code will fit the data using the `chocolate_train` dataset which we partitioned from the original dataset. This will look at the variable `final_grade` in relation to `cocoa_percent` and `bean_type`.  

```{r}

# Train the model
model <- model_spec %>%
  fit(formula = final_grade ~ cocoa_percent + bean_type,
      data = chocolate_train)

# Information about the model
model

```  

### Predict the Data  
The following code will use the dataset `chocolate_test` to make predictions about `final_grade` using the model that was built.  

```{r}

# Make predictions with the model.
predictions <- predict(model,
                       new_data = chocolate_test) %>%
  bind_cols(chocolate_test)

predictions

```  


### Visualization  

```{r}
# Visualization of the model
model$fit %>% rpart.plot(box.palette="RdBu", shadow.col="gray", nn=TRUE)

```  

### Analysis  
Surprisingly, the `cocoa_percent` did not have a strong influence on the `final_grade`. A `cocoa_percent` greater than or equal to 0.91, resulted in a rating of 2.3. The grade also depended upon the type of bean: Amazon mix,Amazon, ICS,Beniano,Blend-Forastero,Criollo,CCN51,Criollo (Ocumare 77),Criollo (Wild),Criollo, +,Criollo, Forastero,EET,Forastero (Amelonado),Forastero (Arriba) ASSS,Forastero (Catongo),Forastero (Parazinho),Matina,Trinitario (85% Criollo),Trinitario, TCGA 27.



## Constructing the Second Model

I will construct the second model for a Decision Tree regression tree using the training dataset.


### Fit the Data  
The following code will fit the data using the `chocolate_train` dataset which we partitioned from the original dataset. This will look at the variable `final_grade` in relation to `cocoa_percent`, `bean_type`, `broad_bean_origin`, and `review_date`.  

```{r}

# Train the model
model2 <- model_spec %>%
  fit(formula = final_grade ~ cocoa_percent + bean_type + broad_bean_origin + review_date,
      data = chocolate_train)

# Information about the model
model2

```  

### Predict the Data  
The following code will use the dataset `chocolate_test` to make predictions about `final_grade` using the model that was built.  

```{r}

# Make predictions with the model.
predictions <- predict(model2,
                       new_data = chocolate_test) %>%
  bind_cols(chocolate_test)

predictions

```  


### Visualization  

```{r}
# Visualization of the model
model2$fit %>% rpart.plot(box.palette="RdBu", shadow.col="gray", nn=TRUE)

```  


### Analysis  

Overall, the `final_grade` came down to `bean_type` and `cocoa_percent`. The year and other variables contributed noise. 


# Random Forest  

## Creating the Model  

The following code create a random forest model.  

```{r}
# creates the random forest model
(chocolate_rf <- ranger(final_grade ~ bean_type + cocoa_percent + broad_bean_origin,
                         chocolate_train,
                         num.trees=500,
                         respect.unordered.factors = "order",
                         seed=32022))

```  


## Predictions and RMSE  

The following code runs predictions using the random forest model and the RMSE (Root Mean Square Error).

```{r}
# Create a column called pred to store the prediction from the random forest model
chocolate_test$pred <- predict(chocolate_rf, chocolate_test)$predictions

# Calculate the RMSE of the predictions
chocolate_test %>% 
  mutate(residual = final_grade - pred) %>%
  summarize(rmse = sqrt(mean(residual^2)))

# Plot the actual outcome vs predictions (prediction on the x-axis)
ggplot(chocolate_test, aes(x = pred, y = final_grade)) +
  geom_point() + 
  geom_abline()

  
```  

# Summary  
The decision trees were quick to compute and provided preliminary information. However, care had to be taken in splitting the train and test data because the system would throw errors for new variables. I did find it more difficult to use the decision tree to do a continuous prediction as opposed to a regular classification.



